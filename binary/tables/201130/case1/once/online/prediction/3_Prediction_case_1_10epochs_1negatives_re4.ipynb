{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <font color='red'>GPU Seeting</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version :  2.3.1\n",
      "keras version :  2.4.0\n",
      "WARNING:tensorflow:From <ipython-input-1-a141e1751472>:18: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU available ? :  True\n"
     ]
    }
   ],
   "source": [
    "# gpu number setting\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "# Gpu growth setting\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "\n",
    "# tensorflow & keras version check\n",
    "print('tensorflow version : ' , tf.__version__)\n",
    "print('keras version : ' , tf.keras.__version__)\n",
    "\n",
    "# tensorflow gpu available check \n",
    "print('GPU available ? : ', tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <font color='red'>Import</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\n",
      "DeepCTR version 0.8.3 detected. Your version is 0.8.2.\n",
      "Use `pip install -U deepctr` to upgrade.Changelog: https://github.com/shenweichen/DeepCTR/releases/tag/v0.8.3\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tqdm\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_columns = 50\n",
    "\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, MultiLabelBinarizer\n",
    "\n",
    "from deepctr.models import DeepFM\n",
    "from deepctr.layers import custom_objects\n",
    "from deepctr.feature_column import SparseFeat,DenseFeat, get_feature_names, VarLenSparseFeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = './debug3_false/'\n",
    "\n",
    "# model_weight_dir = os.path.join(root_path, 'model_weight')\n",
    "rec_list_dir = os.path.join(root_path, 'rec_list')\n",
    "\n",
    "# model weight save path\n",
    "# if not os.path.exists(model_weight_dir) :\n",
    "#     os.makedirs(model_weight_dir)\n",
    "\n",
    "# rec_list_dir save path\n",
    "if not os.path.exists(rec_list_dir) :\n",
    "    os.makedirs(rec_list_dir)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <font color='red'>데이터</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../../../../../../../../Data/movie_201130_table_3_4_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['sa_id', 'album_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 type 변경\n",
    "data['sa_id'] = data['sa_id'].astype(str, copy=True)\n",
    "data['album_id'] = data['album_id'].astype(str, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>label encoding </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../lbe_dict.pickle', 'rb') as f:\n",
    "    lbe_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbe_sa_id = lbe_dict['sa_id']\n",
    "data['sa_id'] = lbe_sa_id.fit_transform(data['sa_id'])\n",
    "\n",
    "lbe_album_id = lbe_dict['album_id']\n",
    "data['album_id'] = lbe_album_id.fit_transform(data['album_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred_dir = './debug3_false/rec_list/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_pred_dir,'unique_uid.csv'),'a') as out_file:\n",
    "    np.savetxt(out_file, np.vstack((lbe_sa_id.classes_,list(range(len(lbe_sa_id.classes_))))).T, delimiter=',', fmt=['%s','%i'])        \n",
    "\n",
    "with open(os.path.join(model_pred_dir,'unique_sid.csv'),'a') as out_file:\n",
    "    np.savetxt(out_file, np.vstack((lbe_album_id.classes_,list(range(len(lbe_album_id.classes_))))).T, delimiter=',', fmt=['%s','%i'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = ['sa_id', 'album_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 변수 설정\n",
    "target = ['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train _ val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2.count #unique features for each sparse field\n",
    "fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique(),embedding_dim=4) for feat in sparse_features]\n",
    "    \n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.generate input data for model\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=2020)\n",
    "train_model_input = {name:train[name].values for name in feature_names}\n",
    "test_model_input = {name:test[name].values for name in feature_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Define Model,train,predict and evaluate\n",
    "model = tf.keras.models.load_model('../../../../../../debug/debug2/model/', custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Define Model,train,predict and evaluate\n",
    "model = tf.keras.models.load_model('../../model/said_albumid_onlineNegative/case1_re4/10_epochs_01_negative/', custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = model(test_model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2894828, 1), dtype=float32, numpy=\n",
       "array([[0.99777055],\n",
       "       [0.89596343],\n",
       "       [0.926893  ],\n",
       "       ...,\n",
       "       [0.78797835],\n",
       "       [0.9872869 ],\n",
       "       [0.085072  ]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsa_id = train['sa_id'].nunique()\n",
    "nalbum_id = train['album_id'].nunique()\n",
    "album_array = np.sort(train['album_id'].unique())\n",
    "topk = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_user(num_users, batch_size=1):\n",
    "    user_list = np.sort(train['sa_id'].unique())\n",
    "    for idx in np.arange(0, num_users, batch_size):\n",
    "        yield user_list[idx : min(idx + batch_size, num_users)]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4999/4999 [39:05<00:00,  2.13it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('./debug3_false/rec_list/rec.csv','a') as out_file:\n",
    "    with tqdm.tqdm(total=math.ceil(nsa_id/300)) as pbar:\n",
    "        for batch_u in batch_user(nsa_id, 300):\n",
    "            temp_batch_sa = np.repeat(batch_u, nalbum_id)\n",
    "            temp_batch_album = np.tile(album_array, len(batch_u))\n",
    "            \n",
    "            temp_batch_input = {'sa_id' : temp_batch_sa, 'album_id' : temp_batch_album}\n",
    "            \n",
    "            #pred = model.predict_on_batch(temp_batch_input)\n",
    "            pred = model(temp_batch_input).numpy()\n",
    "            pred_mat = pred.reshape(-1,nalbum_id)\n",
    "            \n",
    "            user_idx = 0\n",
    "            \n",
    "            for temp_pred in pred_mat:\n",
    "                #b = tf.argsort(temp_pred, axis=-1,direction='DESCENDING',stable=False,name=None)\n",
    "                #c = tf.keras.backend.eval(b)[:topk]\n",
    "                temp_topk = np.argsort(temp_pred)[::-1][:topk]\n",
    "                temp_score = temp_pred[temp_topk]   \n",
    "                temp_df = np.column_stack((np.full(topk, batch_u[user_idx]),temp_topk, temp_score))\n",
    "                \n",
    "                \n",
    "                fmt = ','.join(['%i','%i','%1.5f'])\n",
    "                fmt = '\\n'.join([fmt]*temp_df.shape[0])\n",
    "                data = fmt % tuple(temp_df.ravel())      \n",
    "                out_file.write(data)\n",
    "                #np.savetxt(out_file, temp_df, delimiter=',', fmt=['%i','%i','%1.5f'])\n",
    "\n",
    "                user_idx += 1\n",
    "            \n",
    "            pbar.update(1)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2_p36]",
   "language": "python",
   "name": "conda-env-tensorflow2_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
